{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-I8PKyUIZh_",
        "outputId": "f4b60cf9-c620-4bfb-c6f5-e1577fee7b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hdeb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 338 kB in 3s (131 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "4 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.3.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "--2023-09-07 18:25:06--  http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n",
            "Resolving nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)... 185.125.190.39, 185.125.190.36, 91.189.91.83, ...\n",
            "Connecting to nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)|185.125.190.39|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1321244 (1.3M) [application/x-debian-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb.1’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M  1.32MB/s    in 1.0s    \n",
            "\n",
            "2023-09-07 18:25:07 (1.32 MB/s) - ‘libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb.1’ saved [1321244/1321244]\n",
            "\n",
            "(Reading database ... 120913 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2.19) over (1.1.1f-1ubuntu2.19) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2.19) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "4 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# 2V25saC0pDFc8heT7wxsjigW3CF_32oQPdVZZP9wSMGd29FCa\n",
        "# 2NqtkyBMF0KF99ofEU1fGz1pCJS_3XehxpoU47JGBiqY6tV7M\n",
        "\n",
        "!pip install gradio_client paddleocr paddlepaddle paddlepaddle-gpu deepface transformers TTS openai-whisper pyngrok fastapi[all] nest_asyncio -q\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok authtoken 2V25saC0pDFc8heT7wxsjigW3CF_32oQPdVZZP9wSMGd29FCa\n",
        "!wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n",
        "!sudo apt update && sudo apt upgrade && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynIQOpICITpK",
        "outputId": "824bb8e5-73ea-4860-8319-40af9f5558b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/timing.py:58: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
            "  def backtrace(trace: np.ndarray):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://huggingfaceh4-falcon-chat.hf.space/ ✔\n",
            "[2023/09/07 18:26:06] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/ml/Multilingual_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/devanagari/devanagari_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/dict/devanagari_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, lang='hi', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
            " > tts_models/en/jenny/jenny is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:48000\n",
            " | > resample:False\n",
            " | > num_mels:100\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:2048\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:512\n",
            " | > win_length:2048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3711"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import whisper\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException, Body\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uuid\n",
        "import os\n",
        "import uvicorn\n",
        "import torch\n",
        "from TTS.api import TTS\n",
        "from fastapi.responses import FileResponse\n",
        "import asyncio\n",
        "import gc\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from deepface import DeepFace\n",
        "import paddleocr\n",
        "from gradio_client import Client\n",
        "\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "client = Client(\"https://huggingfaceh4-falcon-chat.hf.space/\", serialize=False)\n",
        "ocr_reader = paddleocr.PaddleOCR(lang=\"hi\",use_gpu=True)\n",
        "voice_model = whisper.load_model(\"large-v2\").to(device)\n",
        "tts = TTS('tts_models/en/jenny/jenny').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicNER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"ai4bharat/IndicNER\")\n",
        "quantized_ner_model = torch.ao.quantization.quantize_dynamic(model,{torch.nn.Linear},dtype=torch.qint8)\n",
        "del model\n",
        "port=8888\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DtnjO2BFCdhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cde0ebd-ec29-4c43-bdbc-51fd0447f75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-09-07T18:40:26+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://75b0-34-125-191-35.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [21471]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8888 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:bd5c:2069:542e:7f87:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /coqui-tts/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['I want to go from Matunga Station to Borivali Station']\n",
            " > Processing time: 8.840007543563843\n",
            " > Real-time factor: 2.5149381347265556\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 292, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 122, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 91, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 146, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
            "    await self.app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 718, in __call__\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 276, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 66, in app\n",
            "    response = await func(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 273, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"<ipython-input-4-88f95794344f>\", line 114, in falcon\n",
            "    result = client.predict(\n",
            "IndexError: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2405:201:35:eb5d:4a53:af0c:3b57:b372:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 292, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 122, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 91, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 146, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
            "    await self.app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 718, in __call__\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 276, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 66, in app\n",
            "    response = await func(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 273, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"<ipython-input-4-88f95794344f>\", line 114, in falcon\n",
            "    result = client.predict(\n",
            "IndexError: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 292, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 122, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 91, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 146, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
            "    await self.app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 718, in __call__\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 276, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 66, in app\n",
            "    response = await func(request)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 273, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 190, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "  File \"<ipython-input-4-88f95794344f>\", line 114, in falcon\n",
            "    result = client.predict(\n",
            "IndexError: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['I want to go from the other station to Boreveli station right now.']\n",
            " > Processing time: 0.6121945381164551\n",
            " > Real-time factor: 0.15658484221582106\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['How do i go from Matunga to borivali station']\n",
            " > Processing time: 0.6053035259246826\n",
            " > Real-time factor: 0.1987425388830084\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 422 Unprocessable Entity\n",
            " > Text splitted to sentences.\n",
            "['To go from Matunga to Borivali station, you can take a train from Matunga station to Borivali station.', 'The train route is as follows: Matunga - Dadar - Mahim - Bandra - Khar Road - Santacruz - Vile Parle - Andheri - Jogeshwari - Goregaon - Malad - Kandivali - Borivali.', 'Alternatively, you can take a bus from Matunga to Borivali station.', 'The bus route is as follows: Matunga - Mahim - Bandra - Khar Road - Santacruz - Vile Parle - Andheri - Jogeshwari - Goregaon - Malad - Kandivali - Borivali.']\n",
            " > Processing time: 4.362593412399292\n",
            " > Real-time factor: 0.17019220074379554\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:172f:de62:b26b:e3c9:d149:2e:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['The distance between Boreveli station and Matunga station is 6.3 km.', 'You can take a train from Boreveli station to Matunga station.', 'The train takes approximately 13 minutes to reach Matunga station.', 'Alternatively, you can take a bus from Boreveli station to Matunga station.', 'The bus takes approximately 23 minutes to reach Matunga station.', 'Do you have any further questions?']\n",
            " > Processing time: 3.5201292037963867\n",
            " > Real-time factor: 0.14864155070502436\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['You can take a train from Matunga to Borivali.']\n",
            " > Processing time: 0.43828868865966797\n",
            " > Real-time factor: 0.16017371981715237\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['I can help you with anything you need.', 'Please let me know what you need help with.']\n",
            " > Processing time: 0.7419383525848389\n",
            " > Real-time factor: 0.15734589691464135\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['There are several ways to get from Matunga Station to Borivali Station.', 'You can take a local train from Matunga Station to Mahim Station, then take a Western Line train from Mahim Station to Borivali Station.', 'Alternatively, you can take a bus from Matunga Station to Borivali Station.', 'There are also several auto-rickshaw and taxi services available.', 'Which mode of transportation would you prefer?']\n",
            " > Processing time: 3.5811073780059814\n",
            " > Real-time factor: 0.1505742495902948\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "[\"I'm sorry, I cannot understand your question as it is in a different language.\", 'Can you please try to ask the question in English?']\n",
            " > Processing time: 1.0044431686401367\n",
            " > Real-time factor: 0.12916114470297516\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['Hi User, I am Falcon, an AI assistant built to help you with your queries.', 'How can I assist you today?']\n",
            " > Processing time: 0.8620162010192871\n",
            " > Real-time factor: 0.1311782795504647\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['I am an AI assistant, and I can help you with a wide range of tasks, such as answering your questions, providing you with information, scheduling your appointments, and more.', 'What can I assist you with?']\n",
            " > Processing time: 1.4182674884796143\n",
            " > Real-time factor: 0.1326889061759759\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 422 Unprocessable Entity\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['Sure, I can help you with that.', 'Do you want me to give you the quickest route or the cheapest route?']\n",
            " > Processing time: 0.8259704113006592\n",
            " > Real-time factor: 0.14918189246851157\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "[\"I'm sorry, I don't have any information on the current weather in Mumbai.\", 'Could you please provide me with the current date and location?']\n",
            " > Processing time: 0.926764726638794\n",
            " > Real-time factor: 0.12184653255834788\n",
            "INFO:     2401:4900:172f:de62:b26b:e3c9:d149:2e:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "[\"I'm sorry, I don't understand the question.\", 'Could you please provide more context or rephrase the question?']\n",
            " > Processing time: 0.8783154487609863\n",
            " > Real-time factor: 0.13131398117626628\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['Yes, Dadar is a neighborhood located in the city of Mumbai, Maharashtra, India.']\n",
            " > Processing time: 0.6705896854400635\n",
            " > Real-time factor: 0.13680850434003336\n",
            "INFO:     171.51.200.159:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     171.51.200.159:0 - \"POST /ner/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"POST /chat/ HTTP/1.1\" 200 OK\n",
            "INFO:     49.36.101.103:0 - \"OPTIONS /coqui-tts/ HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "[\"I'm doing great, thanks.\", 'How are you?']\n",
            " > Processing time: 0.6015689373016357\n",
            " > Real-time factor: 0.18247793851414632\n",
            "INFO:     49.36.101.103:0 - \"POST /coqui-tts/ HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [21471]\n"
          ]
        }
      ],
      "source": [
        "app = FastAPI(title=\"SIH 2023 Backend\")\n",
        "\n",
        "app.add_middleware(\n",
        "CORSMiddleware,\n",
        "allow_origins=[\"*\"],\n",
        "allow_credentials=True,\n",
        "allow_methods=[\"*\"],\n",
        "allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class TTSNER(BaseModel):\n",
        "    text: str\n",
        "    emotion: str = \"Cheerful & Professional\"\n",
        "\n",
        "class FaceResponse(BaseModel):\n",
        "    prediction: bool\n",
        "\n",
        "class FaceDetect(BaseModel):\n",
        "    prediction: str\n",
        "\n",
        "def remove(known_image_path,test_image_path):\n",
        "    os.remove(known_image_path)\n",
        "    os.remove(test_image_path)\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def home():\n",
        "    return \"SIH - LICHT DEN CODE\"\n",
        "\n",
        "@app.post(\"/face-detect/\")\n",
        "async def face_detect(img: UploadFile = File(...)):\n",
        "    img_path = f\"{uuid.uuid4()}.jpg\"\n",
        "    with open(img_path, \"wb\") as known_image_file:\n",
        "        known_image_file.write(img.file.read())\n",
        "    result = DeepFace.extract_faces(img_path,enforce_detection=False)[0].get('confidence')\n",
        "    os.remove(img_path)\n",
        "    if result>5:\n",
        "        return FaceDetect(prediction=\"Done!!!\")\n",
        "    else:\n",
        "        return FaceDetect(prediction=\"Face could not be detected. Please confirm that the picture is a face photo.\")\n",
        "\n",
        "@app.post(\"/face-match/\")\n",
        "async def face_match(known_face: UploadFile = File(...), test_face: UploadFile = File(...)):\n",
        "    try:\n",
        "        known_image_path = f\"{uuid.uuid4()}.jpg\"\n",
        "        test_image_path = f\"{uuid.uuid4()}.jpg\"\n",
        "        with open(known_image_path, \"wb\") as known_image_file:\n",
        "            known_image_file.write(known_face.file.read())\n",
        "        with open(test_image_path, \"wb\") as test_image_file:\n",
        "            test_image_file.write(test_face.file.read())\n",
        "        result = DeepFace.verify(known_image_path, test_image_path, model_name='Facenet512', distance_metric='euclidean_l2').get('verified')\n",
        "        remove(known_image_path,test_image_path)\n",
        "        return FaceResponse(prediction=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        remove(known_image_path,test_image_path)\n",
        "        if str(e)=='''Face could not be detected. Please confirm that the picture is a face photo or consider to set enforce_detection param to False.''':\n",
        "            raise HTTPException(status_code=399, detail=\"Face could not be detected. Please confirm that the picture is a face photo.\")\n",
        "        else:\n",
        "            raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/transcribe/\")\n",
        "async def transcribe_audio(file: UploadFile = File(...)):\n",
        "    audio_path = f\"{uuid.uuid4()}.webm\"\n",
        "    with open(audio_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    result = voice_model.transcribe(whisper.pad_or_trim(whisper.load_audio(audio_path)))[\"text\"]\n",
        "    os.remove(audio_path)\n",
        "    return {\"text\": result}\n",
        "\n",
        "@app.post(\"/ocr/\")\n",
        "async def OCR(file: UploadFile = File(...)):\n",
        "    pic_path = f\"{uuid.uuid4()}.jpg\"\n",
        "    with open(pic_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    result = ' '.join([word[1][0] for line in ocr_reader.ocr(pic_path) for word in line])\n",
        "    os.remove(pic_path)\n",
        "    return {\"text\": result}\n",
        "\n",
        "@app.post(\"/ner/\")\n",
        "async def get_ner_endpoint(request: TTSNER = Body(...)):\n",
        "    sentence = request.text.strip()\n",
        "    tok_sentence = tokenizer(sentence, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        logits = quantized_ner_model(**tok_sentence).logits.argmax(-1)\n",
        "        predicted_tokens_classes = [\n",
        "            quantized_ner_model.config.id2label[t.item()] for t in logits[0]]\n",
        "\n",
        "        predicted_labels = []\n",
        "\n",
        "        previous_token_id = 0\n",
        "        word_ids = tok_sentence.word_ids()\n",
        "        for word_index in range(len(word_ids)):\n",
        "            if word_ids[word_index] == None:\n",
        "                previous_token_id = word_ids[word_index]\n",
        "            elif word_ids[word_index] == previous_token_id:\n",
        "                previous_token_id = word_ids[word_index]\n",
        "            else:\n",
        "                predicted_labels.append(predicted_tokens_classes[word_index])\n",
        "                previous_token_id = word_ids[word_index]\n",
        "\n",
        "    return {\"LOC\": [word for word, label in zip(sentence.split(' '), predicted_labels) if \"LOC\" in label]}\n",
        "\n",
        "@app.post(\"/coqui-tts/\")\n",
        "async def coqui_tts(request: TTSNER = Body(...)):\n",
        "    out = f\"{uuid.uuid4()}.ogg\"\n",
        "    async def remove():\n",
        "        loop = asyncio.get_event_loop()\n",
        "        await loop.run_in_executor(None, lambda: os.remove(out))\n",
        "    tts.tts_to_file(request.text, file_path=out, emotion=request.emotion)\n",
        "    return FileResponse(out,headers={\"Content-Disposition\":f\"attachment; filename={out}\"},background=remove)\n",
        "\n",
        "@app.post(\"/chat/\")\n",
        "async def falcon(request: TTSNER = Body(...)):\n",
        "  result = client.predict(\n",
        "  request.text, # str in 'Type an input and press Enter' Textbox component\n",
        "  \"\", # str (filepath to JSON file) in 'parameter_9' Chatbot component\n",
        "  \"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon was built by the Technology Innovation Institute in Abu Dhabi. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\", # str in 'Instructions' Textbox component\n",
        "  0.7, # int | float (numeric value between 0.1 and 2.0) in 'Temperature' Slider component\n",
        "  0.8, # int | float (numeric value between 0.1 and 0.99) in 'p (nucleus sampling)' Slider component\n",
        "  fn_index=1)[0][1]\n",
        "  return {\"text\":result}\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    ngrok_tunnel = ngrok.connect(port)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app,port=port)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}